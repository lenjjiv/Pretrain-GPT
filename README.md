# Дообучение GPT-2 на русском языке

Проект реализует дообучение модели GPT-2 на русскоязычных данных с использованием датасета Alpaca.

## Установка

1. Клонируйте репозиторий:
```bash
git clone https://github.com/lenjjiv/Pretrain-GPT.git
cd Pretrain-GPT
```

2. Создайте виртуальное окружение и активируйте его:
```bash
python -m venv venv
source venv/bin/activate  # для Linux/MacOS
# или
venv\Scripts\activate     # для Windows
```

3. Установите зависимости:
```bash
pip install -r requirements.txt
```

## Использование

### Обучение

Для запуска процесса обучения:

```bash
python scripts/train.py
```

Это запустит полный пайплайн:
1. Подготовку токенизатора
2. Инициализацию модели
3. Загрузку и подготовку датасета
4. Обучение модели
5. Сохранение результатов

### Генерация текста

После обучения вы можете использовать модель для генерации текста:

```python
from src.generator import TextGenerator

generator = TextGenerator(
    model_path="./gpt2_finetuned_final",
    device=0  # используйте -1 для CPU
)

text = generator.generate(
    prompt="some_text",
    max_length=100,
    temperature=0.8
)
print(text)
```

## Параметры обучения

Основные параметры обучения можно настроить в `src/trainer.py`:

- Количество эпох: 5
- Размер батча: 16
- Скорость обучения: 5e-5
- Шаги накопления градиента: 10
- Максимальная длина последовательности: 512 токенов

## Технические детали

- Базовая модель: GPT-2
- Датасет: Alpaca Russian
- Размер словаря: 20,000 токенов
- Размер эмбеддингов: 384 (768/2)
- Количество слоев: 12
- Количество голов внимания: 12

## Структура кода

- `tokenizer.py`: Содержит логику подготовки и обработки токенизатора, включая нормализацию словаря и удаление дубликатов
- `dataset.py`: Реализует загрузку и подготовку датасета, включая потоковую обработку данных
- `model.py`: Определяет архитектуру и конфигурацию модели
- `trainer.py`: Содержит логику обучения и настройки параметров
- `generator.py`: Предоставляет интерфейс для генерации текста
